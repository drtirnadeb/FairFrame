{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7551354b",
   "metadata": {},
   "source": [
    "# üßÆ Fairness Analysis ‚Äì Intermediate Level\n",
    "\n",
    "This notebook explores fairness across machine learning models using two datasets:\n",
    "- `loan_data.csv`: Loan approval dataset with injected caste and religion bias.\n",
    "- `promotion_data.csv`: Account promotion dataset with exposure bias.\n",
    "\n",
    "We compare models **with and without protected attributes**, and compute basic group fairness metrics like **Disparate Impact Ratio**.\n",
    "\n",
    "### üõ°Ô∏è Protected Attributes\n",
    "- `gender`\n",
    "- `caste`\n",
    "- `religion`\n",
    "- `accent_score`\n",
    "\n",
    "### üßÆ Neutral Attributes\n",
    "- `age`\n",
    "- `income`\n",
    "- `credit_history_score`\n",
    "- `number_of_loans`\n",
    "- `savings_balance`\n",
    "- `years_with_bank`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10464730-11b8-4a72-be40-ceaa2b82e811",
   "metadata": {},
   "source": [
    "## üß™ Intermediate Challenge Instructions\n",
    "\n",
    "In this level, your goal is to explore **group-level disparities** in both **loan approval** and **promotion exposure**, combining EDA and fairness-aware ML modeling.\n",
    "\n",
    "---\n",
    "\n",
    "### üìò About the Notebook\n",
    "\n",
    "We have included an example Jupyter notebook (`intermediate_fairness_analysis.ipynb`) to help you get started.  \n",
    "Use it as a **template** ‚Äî feel free to build on it, expand the analysis, and apply the same techniques to the promotion dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### üîç 1. Exploratory Data Analysis (EDA)\n",
    "\n",
    "Begin with data understanding:\n",
    "- Explore distributions of **protected attributes**: `caste`, `religion`, `gender`\n",
    "- Examine **neutral features**: `credit_history_score`, `number_of_loans`, `savings_balance`, etc.\n",
    "- Visualize group-level statistics using:\n",
    "  - Bar plots\n",
    "  - Countplots\n",
    "  - Correlation heatmaps\n",
    "\n",
    "‚úÖ Tip: Replicate or build on the visualizations provided in the Beginner notebook to support your fairness analysis.\n",
    "\n",
    "---\n",
    "\n",
    "### ü§ñ 2. Train and Compare ML Models (Loan Dataset)\n",
    "\n",
    "Use the `loan_df` to predict the `approved` outcome.\n",
    "\n",
    "Train **two models**:\n",
    "- **With protected attributes** (e.g., `caste`, `religion`, `gender`)\n",
    "- **Without protected attributes** (only neutral features)\n",
    "\n",
    "Suggested models:\n",
    "- Logistic Regression\n",
    "- Decision Tree Classifier\n",
    "\n",
    "Evaluate model performance using:\n",
    "- Accuracy\n",
    "- Confusion Matrix\n",
    "\n",
    "‚ö†Ô∏è Note: This is a synthetic dataset, so perfect accuracy may indicate it's too simple or that bias is encoded in correlated non-protected features.\n",
    "\n",
    "---\n",
    "\n",
    "### üìä 3. Fairness Metrics (Loan + Promotion)\n",
    "\n",
    "Calculate **group fairness metrics** for both `loan_df` and `promo_df`:\n",
    "\n",
    "#### For Loan Dataset:\n",
    "- **Disparate Impact Ratio (DIR)**: Compare approval rates between groups (e.g., SC/ST vs General)\n",
    "- **Equal Opportunity Difference**: Difference in true positive rates across groups\n",
    "- **Accuracy by Group**\n",
    "\n",
    "#### For Promotion Dataset:\n",
    "- Treat `assigned_promotion != \"No_Promo\"` as the positive class (received benefit)\n",
    "- Repeat similar fairness metrics (DIR, accuracy, etc.) by `caste` or `religion`\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Final Task\n",
    "\n",
    "Summarize your findings:\n",
    "- Do protected groups receive systematically different outcomes?\n",
    "- Does removing protected features improve fairness?\n",
    "- How does fairness vary between **loan approval** and **promotion exposure** tasks?\n",
    "\n",
    "You are encouraged to structure your analysis with clear sections and interpretations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "152aed19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>gender</th>\n",
       "      <th>caste</th>\n",
       "      <th>religion</th>\n",
       "      <th>age</th>\n",
       "      <th>income</th>\n",
       "      <th>accent_score</th>\n",
       "      <th>credit_history_score</th>\n",
       "      <th>number_of_loans</th>\n",
       "      <th>savings_balance</th>\n",
       "      <th>years_with_bank</th>\n",
       "      <th>loan_score</th>\n",
       "      <th>approved</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U000</td>\n",
       "      <td>Male</td>\n",
       "      <td>SC/ST</td>\n",
       "      <td>Other</td>\n",
       "      <td>41</td>\n",
       "      <td>65247</td>\n",
       "      <td>54.84</td>\n",
       "      <td>739.0</td>\n",
       "      <td>2</td>\n",
       "      <td>47828</td>\n",
       "      <td>7</td>\n",
       "      <td>65.97</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U001</td>\n",
       "      <td>Female</td>\n",
       "      <td>OBC</td>\n",
       "      <td>Hindu</td>\n",
       "      <td>52</td>\n",
       "      <td>92752</td>\n",
       "      <td>96.98</td>\n",
       "      <td>707.6</td>\n",
       "      <td>2</td>\n",
       "      <td>48924</td>\n",
       "      <td>17</td>\n",
       "      <td>55.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>U002</td>\n",
       "      <td>Male</td>\n",
       "      <td>SC/ST</td>\n",
       "      <td>Hindu</td>\n",
       "      <td>43</td>\n",
       "      <td>86573</td>\n",
       "      <td>77.37</td>\n",
       "      <td>638.4</td>\n",
       "      <td>0</td>\n",
       "      <td>75182</td>\n",
       "      <td>7</td>\n",
       "      <td>71.31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U003</td>\n",
       "      <td>Male</td>\n",
       "      <td>SC/ST</td>\n",
       "      <td>Hindu</td>\n",
       "      <td>53</td>\n",
       "      <td>89101</td>\n",
       "      <td>63.78</td>\n",
       "      <td>709.5</td>\n",
       "      <td>1</td>\n",
       "      <td>94065</td>\n",
       "      <td>14</td>\n",
       "      <td>89.54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>U004</td>\n",
       "      <td>Male</td>\n",
       "      <td>OBC</td>\n",
       "      <td>Other</td>\n",
       "      <td>23</td>\n",
       "      <td>56646</td>\n",
       "      <td>64.51</td>\n",
       "      <td>770.0</td>\n",
       "      <td>2</td>\n",
       "      <td>60052</td>\n",
       "      <td>15</td>\n",
       "      <td>71.09</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  user_id  gender  caste religion  age  income  accent_score  \\\n",
       "0    U000    Male  SC/ST    Other   41   65247         54.84   \n",
       "1    U001  Female    OBC    Hindu   52   92752         96.98   \n",
       "2    U002    Male  SC/ST    Hindu   43   86573         77.37   \n",
       "3    U003    Male  SC/ST    Hindu   53   89101         63.78   \n",
       "4    U004    Male    OBC    Other   23   56646         64.51   \n",
       "\n",
       "   credit_history_score  number_of_loans  savings_balance  years_with_bank  \\\n",
       "0                 739.0                2            47828                7   \n",
       "1                 707.6                2            48924               17   \n",
       "2                 638.4                0            75182                7   \n",
       "3                 709.5                1            94065               14   \n",
       "4                 770.0                2            60052               15   \n",
       "\n",
       "   loan_score  approved  \n",
       "0       65.97         1  \n",
       "1       55.00         0  \n",
       "2       71.31         1  \n",
       "3       89.54         1  \n",
       "4       71.09         1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load loan data\n",
    "loan_df = pd.read_csv(\"data/loan_data.csv\")\n",
    "promo_df = pd.read_csv('./data/promotion_data.csv')\n",
    "\n",
    "\n",
    "# Preview\n",
    "loan_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b7d0f7",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Encode categorical features and define X/y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe0b058d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables\n",
    "loan_df_encoded = pd.get_dummies(loan_df.drop(columns=['user_id']), drop_first=True)\n",
    "\n",
    "# Target variable\n",
    "y = loan_df_encoded['approved']\n",
    "X = loan_df_encoded.drop(columns=['approved'])\n",
    "\n",
    "# Identify protected attribute columns\n",
    "protected_cols = [col for col in X.columns if any(p in col for p in ['gender_', 'caste_', 'religion_', 'accent_score'])]\n",
    "\n",
    "# Neutral-only feature set\n",
    "X_neutral = X.drop(columns=protected_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5429cc74",
   "metadata": {},
   "source": [
    "## ü§ñ Logistic Regression and Decision Tree Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f658962c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/deb/miniconda3/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LogReg w/ Prot</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LogReg w/o Prot</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tree w/ Prot</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tree w/o Prot</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Model  Accuracy\n",
       "0   LogReg w/ Prot      1.00\n",
       "1  LogReg w/o Prot      0.95\n",
       "2     Tree w/ Prot      1.00\n",
       "3    Tree w/o Prot      1.00"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=42)\n",
    "Xn_train, Xn_test = train_test_split(X_neutral, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Logistic Regression with full features\n",
    "lr_full = LogisticRegression(max_iter=1000)\n",
    "lr_full.fit(X_train, y_train)\n",
    "acc_lr_full = accuracy_score(y_test, lr_full.predict(X_test))\n",
    "\n",
    "# Logistic Regression without protected features\n",
    "lr_fair = LogisticRegression(max_iter=1000)\n",
    "lr_fair.fit(Xn_train, y_train)\n",
    "acc_lr_fair = accuracy_score(y_test, lr_fair.predict(Xn_test))\n",
    "\n",
    "# Decision Tree with full features\n",
    "dt_full = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "dt_full.fit(X_train, y_train)\n",
    "acc_dt_full = accuracy_score(y_test, dt_full.predict(X_test))\n",
    "\n",
    "# Decision Tree without protected features\n",
    "dt_fair = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "dt_fair.fit(Xn_train, y_train)\n",
    "acc_dt_fair = accuracy_score(y_test, dt_fair.predict(Xn_test))\n",
    "\n",
    "# Print results\n",
    "pd.DataFrame({\n",
    "    'Model': ['LogReg w/ Prot', 'LogReg w/o Prot', 'Tree w/ Prot', 'Tree w/o Prot'],\n",
    "    'Accuracy': [acc_lr_full, acc_lr_fair, acc_dt_full, acc_dt_fair]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd4f5a6-d599-469a-818b-7d5abb9be15a",
   "metadata": {},
   "source": [
    "**Observations:** Since this is a small synthetic dataset, the models achieve high accuracy even without protected attributes. This suggests either the dataset is too simple or the bias is encoded in correlated non-protected features. **It is important to investigate further whether the models are learning genuine patterns or replicating existing biases.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e597dd",
   "metadata": {},
   "source": [
    "This comparison allows us to evaluate whether using protected attributes **increases model performance** ‚Äî and whether **removing them improves fairness**.\n",
    "\n",
    "> üîé Next Steps: Implement group-level fairness metrics such as:\n",
    "> - **Disparate Impact Ratio**\n",
    "> - **Equal Opportunity Difference**\n",
    "> - **Accuracy by Group**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cab369f-fcf6-40ac-928b-87bae9e75d9e",
   "metadata": {},
   "source": [
    "## üìê Fairness Metrics\n",
    "We compute three metrics using caste as the protected attribute:\n",
    "\n",
    "### 1. **Disparate Impact Ratio (DIR)**\n",
    "Let $P_+^{adv}$ be the favorable outcome rate for the advantaged group, and $P_+^{dis}$ for the disadvantaged group:\n",
    "\n",
    "$$ DIR = \\frac{P_+^{dis}}{P_+^{adv}} $$\n",
    "\n",
    "### 2. **Equal Opportunity Difference (EOD)**\n",
    "True positive rate (TPR) for each group:\n",
    "\n",
    "$$ EOD = TPR_{dis} - TPR_{adv} $$\n",
    "\n",
    "### 3. **Group Accuracy**\n",
    "Let $Acc_g$ be accuracy for group $g$:\n",
    "\n",
    "$$ Acc_g = \\frac{TP + TN}{TP + TN + FP + FN} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0f3461c-9fb1-4de3-b483-7ce73be8f805",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dir_robust(df, protected_col, outcome_col, adv_value, dis_value):\n",
    "    # Normalize values to lowercase and strip whitespace\n",
    "    df = df.copy()\n",
    "    df[protected_col] = df[protected_col].str.strip().str.lower()\n",
    "    adv_value = adv_value.strip().lower()\n",
    "    dis_value = dis_value.strip().lower()\n",
    "\n",
    "    # Compute mean outcome for advantaged and disadvantaged groups\n",
    "    p_adv = df[df[protected_col] == adv_value][outcome_col].mean()\n",
    "    p_dis = df[df[protected_col] == dis_value][outcome_col].mean()\n",
    "\n",
    "    # Return ratio with handling for zero or NaN\n",
    "    if pd.isna(p_adv) or p_adv == 0:\n",
    "        return None  # or return np.nan or raise a warning\n",
    "    return round(p_dis / p_adv, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ddd5863-6f2b-40b5-a924-186c34b72f6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.688"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_dir_robust(loan_df, 'caste', 'approved', 'General', 'SC/ST')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91ca8a2-5fb1-4dc0-a482-135c696e3ea9",
   "metadata": {},
   "source": [
    "**Observations:** This means that individuals from the SC/ST caste group are approved for loans at only 68.8% the rate of individuals from the General caste group.\n",
    "\n",
    "**Interpretation:** A value below 0.80 is commonly considered a threshold for potential disparate impact (known as the \"four-fifths rule\"). At 0.688, this example is below the threshold, indicating a possible bias that deserves further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0915cc-8bf6-4bd4-8d9e-e1e025a70ef4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
